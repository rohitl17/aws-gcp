{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, glob, os, cv2\n",
    "import pydicom\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import requests, time\n",
    "import pandas as pd\n",
    "#from gstorage import upload_blob,download_blob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Storage Environment and Import Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"/test.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import storage\n",
    "\n",
    "\n",
    "# Instantiates a client\n",
    "storage_client = storage.Client()\n",
    "\n",
    "# The name for the new bucket\n",
    "bucket_name = ''\n",
    "\n",
    "def create_bucket(bucket_name):\n",
    "    # Creates the new bucket\n",
    "    bucket = storage_client.create_bucket(bucket_name)\n",
    "    print('Bucket {} created.'.format(bucket.name))\n",
    "    return bucket\n",
    "\n",
    "\n",
    "def upload_blob(bucket_name, source_file_name, destination_blob_name):\n",
    "    \"\"\"Uploads a file to the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(source_file_name)\n",
    "\n",
    "    print('File {} uploaded to {}.'.format(\n",
    "    source_file_name,\n",
    "    destination_blob_name))\n",
    "\n",
    "    \n",
    "    \n",
    "def download_blob(bucket_name, source_blob_name, destination_file_name):\n",
    "    \"\"\"Downloads a blob from the bucket.\"\"\"\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.get_bucket(bucket_name)\n",
    "    blob = bucket.blob(source_blob_name)\n",
    "\n",
    "    blob.download_to_filename(destination_file_name)\n",
    "\n",
    "    print('Blob {} downloaded to {}.'.format(\n",
    "        source_blob_name,\n",
    "        destination_file_name))      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read CSV and List DICOMs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Already have the Normal DCM paths? Put your paths in normal_dcms list and Jump to JSON processing block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "csv_path = './map.csv'\n",
    "df=pd.read_csv(csv_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dcms=list(df['Dicom Path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clip the paths to keep just the file names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958\n"
     ]
    }
   ],
   "source": [
    "clipped_dcm_names=[]\n",
    "for idx, i in enumerate(all_dcms):\n",
    "    dcm=i.split('/')[-1]\n",
    "    clipped_dcm_names.append(dcm)\n",
    "print (len(clipped_dcm_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Normal from CSV and Upload to the Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "normal_dcms=[]\n",
    "for idx, row in df.iterrows():\n",
    "    region_dict=json.loads(row['region_attributes'])\n",
    "    keys=json.loads(row['region_attributes']).keys()\n",
    "    \n",
    "    dcm_name=row['filename'].split('.png')[0]+'.dcm'\n",
    "    source_path=all_dcms[clipped_dcm_names.index(dcm_name)]\n",
    "    normal_dcms.append(source_path)\n",
    "    upload_blob('buck', source_path, 'test/'+dcm_name)\n",
    "    print (idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "958\n",
      "/opt/bucketdata1/ORIGINAL_DATA/CHAI_original/chai-data/chai/1598 APR 1 TO AUG 31/1598 MAY/190516140117/CHEST_PA_3764.dcm\n"
     ]
    }
   ],
   "source": [
    "print (len(normal_dcms))\n",
    "print (normal_dcms[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
